---
layout: post
title: Распределение прерываний сетевых карт на Linux системах с несколькими физическими процессорами
---

Дано:

```
# lscpu -e
CPU NODE SOCKET CORE L1d:L1i:L2:L3 ONLINE
0   0    0  0    0:0:0:0   да
1   0    0  1    1:1:1:0   да
2   0    0  2    2:2:2:0   да
3   0    0  3    3:3:3:0   да
4   0    1  4    4:4:4:1   да
5   0    1  5    5:5:5:1   да
6   0    1  6    6:6:6:1   да
7   0    1  7    7:7:7:1   да
```

Если прерывания распределены ровно по всем ядрам и лесенка прерываний у нас выглядит так:

```
70:    X  0  0  0  0  0  0  0  IR-PCI-MSI-edge  eth1-TxRx-0
71:    0  X  0  0  0  0  0  0  IR-PCI-MSI-edge  eth1-TxRx-1
72:    0  0  X  0  0  0  0  0  IR-PCI-MSI-edge  eth1-TxRx-2
73:    0  0  0  X  0  0  0  0  IR-PCI-MSI-edge  eth1-TxRx-3
74:    0  0  0  0  X  0  0  0  IR-PCI-MSI-edge  eth1-TxRx-4
75:    0  0  0  0  0  X  0  0  IR-PCI-MSI-edge  eth1-TxRx-5
76:    0  0  0  0  0  0  X  0  IR-PCI-MSI-edge  eth1-TxRx-6
77:    0  0  0  0  0  0  0  X  IR-PCI-MSI-edge  eth1-TxRx-7
```

то с разницей в одну секунду:

```
# ip -s -s link show eth1
    RX errors: length   crc     frame   fifo    missed
               0        0       0       0       125029467

# ip -s -s link show eth1
    RX errors: length   crc     frame   fifo    missed
               0        0       0       0       125029818
```

будем иметь серьёзные потери пакетов. Некошерно как-то. Возможно дело в кэшиках, я не особо вдавался в подробности. Наверное с помощью какого-то суперправильного ethtool flow-control (или как его) можно добиться хороших результатов, но более простой способ - разбить "лесенку" прерываний на две лесенки, полностью попадающие на один физический процессор:

```
70:    0  0  0  0  X  0  0  0  IR-PCI-MSI-edge  eth1-TxRx-0
71:    0  0  0  0  0  X  0  0  IR-PCI-MSI-edge  eth1-TxRx-1
72:    0  0  0  0  0  0  X  0  IR-PCI-MSI-edge  eth1-TxRx-2
73:    0  0  0  0  0  0  0  X  IR-PCI-MSI-edge  eth1-TxRx-3
74:    0  0  0  0  X  0  0  0  IR-PCI-MSI-edge  eth1-TxRx-4
75:    0  0  0  0  0  X  0  0  IR-PCI-MSI-edge  eth1-TxRx-5
76:    0  0  0  0  0  0  X  0  IR-PCI-MSI-edge  eth1-TxRx-6
77:    0  0  0  0  0  0  0  X  IR-PCI-MSI-edge  eth1-TxRx-7
```

Несмотря на то, что нагрузка на конкретные ядра будет больше, обработка пакетов будет, судя по всему, лучше кэшироваться и потерь не будет, либо существенно снизятся.

Пример как сделать такое распределение:

```
tune_eth() {
	local nic=eth1-TxRx-
	local cpucount
	cpucount="$(grep -c 'model name' /proc/cpuinfo)"
	grep "$nic" /proc/interrupts | while read irq $(eval echo cpu{1..$cpucount}) _ queue _; do
		irq=${irq//:}
		proc_entry=/proc/irq/$irq/smp_affinity_list
		evaled="${queue##*TxRx-}"
		[ "$evaled" -gt 4 ] || evaled=$((evaled+4))
		echo "$irq $queue $(cat $proc_entry) -> $evaled"
		echo $evaled > $proc_entry
	done
}
```

Тут важно учитывать как соотносятся логические ядра с физическими процессорами. Не всегда всё ровно или поровну, может быть не только:

```
# lscpu -e
CPU NODE SOCKET CORE L1d:L1i:L2:L3 ONLINE
0   0    0  0    0:0:0:0   да
1   0    0  1    1:1:1:0   да
2   0    0  2    2:2:2:0   да
3   0    0  3    3:3:3:0   да
4   0    1  4    4:4:4:1   да
5   0    1  5    5:5:5:1   да
6   0    1  6    6:6:6:1   да
7   0    1  7    7:7:7:1   да
```

но и

```
# lscpu -e
CPU NODE SOCKET CORE L1d:L1i:L2:L3 ONLINE
0   0    1  0    0:0:0:1   да
1   0    0  1    1:1:1:0   да
2   0    1  2    2:2:2:1   да
3   0    0  3    3:3:3:0   да
4   0    1  4    4:4:4:1   да
5   0    0  5    5:5:5:0   да
6   0    1  6    6:6:6:1   да
7   0    0  7    7:7:7:0   да
```

или даже:

```
# lscpu -e
CPU NODE SOCKET CORE L1d:L1i:L2:L3 ONLINE
0   0    0  0    0:0:0:0   да
1   0    0  1    1:1:1:0   да
2   0    1  2    2:2:2:1   да
3   0    0  3    3:3:3:0   да
4   0    0  4    4:4:4:0   да
5   0    1  5    5:5:5:1   да
6   0    1  6    6:6:6:1   да
7   0    1  7    7:7:7:1   да
```

в таких случаях какой-то более менее стройный математически код распределения прерываний написать тяжелее.

Ещё момент - что делать со вторым простаивающим процессором? Можно разделить трафик идущий на первую карту пополам с ещё одной (либо логически, либо bond) и заставить второй процессор обрабатывать вторую сетевую карту. Должно выйти довольно неплохо, хоть и слегка увеличиться логическая сложность схемы сети.
